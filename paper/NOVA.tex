\documentclass[11pt, a4paper]{article}

\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2cm, right=2cm]{geometry}

\usepackage[italian]{babel}
\usepackage{enumitem}
\setlist[itemize]{label=--}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage[colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue]{hyperref}

\title{\textbf{NOVA: Non-monotonic Orthogonal Variance Activation\\per Architetture Neurali Profonde}\\[0.5em]
\large\textit{Preprint --- Work in Progress}}
\author{Autore\\[0.3em]\textit{Laboratorio di AI e Matematica Applicata}}
\date{24 febbraio 2026}

\begin{document}

\maketitle

\begin{abstract}
La scelta della funzione di attivazione incide significativamente sulla topologia della loss landscape e sulla capacità espressiva delle reti neurali profonde. In questo articolo preliminare introduciamo \textbf{NOVA} (\textit{Non-monotonic Orthogonal Variance Activation}), una funzione ibrida razionale-esponenziale progettata analiticamente. NOVA estende i benefici delle attivazioni gating introducendo un termine di smorzamento razionale che modula la derivata seconda. Dimostriamo analiticamente come NOVA induca una contrazione controllata della varianza all'inizializzazione (richiedendo una scala dei pesi di $\approx 2.8/n_\text{in}$) e formalizziamo il fenomeno della \textit{Covariance Shift Collision}, fornendo una spiegazione teorica per il suo degrado prestazionale in architetture basate su Batch Normalization. In un confronto esplorativo a run singola su Mini-ViT / CIFAR-100 a 100 epoche, NOVA raggiunge il $56.75\%$ di test accuracy, superando GELU ($54.67\%$), ReLU ($54.55\%$), SiLU ($53.84\%$) e Mish ($53.25\%$). Uno studio di scaling su tre varianti di ViT (3.2M--25.3M parametri) mostra un vantaggio crescente di NOVA su GELU (da $+2.28$ a $+4.07$ punti percentuali). Con regolarizzazione DeiT-style (RandAugment, CutMix, Mixup, Stochastic Depth), il vantaggio si amplifica fino a $+8.35$ pp alla scala Small ($62.66\%$ vs $54.31\%$) e l'accuracy alla scala Base recupera $+5.09$ pp, suggerendo una forte complementarità tra la regolarizzazione implicita di NOVA e quella esplicita. Una validazione cross-dataset su Tiny-ImageNet-200 (200 classi, $64\times64$) conferma il pattern: il vantaggio di NOVA cresce da $+1.75$ (Tiny) a $+6.05$ pp (Base) in Top-1 accuracy, con miglioramenti analoghi in Top-5, rafforzando l'ipotesi che il beneficio di NOVA sia indipendente dal dataset e cresca con la capacità del modello. Un test di generalizzazione architetturale su ConvNeXt (CNN pura, no attention) su CIFAR-100 conferma il vantaggio a tutte le scale ($+0.14$ a $+0.96$ pp, amplificato a $+0.93$--$+1.52$ pp con iperparametri ottimizzati), con $\beta$ convergente a valori inferiori ($\approx 0.23$--$0.30$), suggerendo che il contributo di NOVA sia complementare al bias induttivo convolutivo. Osserviamo trend qualitativi incoraggianti anche su Nano-GPT. Un esperimento su Diffusion Transformer (DiT) DDPM su CIFAR-10 (400 epoche, $\approx$22M parametri) mostra che, eliminando la Batch Normalization a favore di AdaLN, NOVA e GELU raggiungono prestazioni equivalenti (FID $21.74$ vs $21.45$, IS $4.10$ vs $4.11$), confermando che il degrado osservato su U-Net DDPM era attribuibile al conflitto con BatchNorm e non a un limite intrinseco di NOVA nel contesto generativo. Nello \textit{Scientific Machine Learning}, l'espressività heavy-tailed della derivata seconda di NOVA ha mostrato una riduzione dell'errore residuo sulle Physics-Informed Neural Networks (PINN) di un fattore $\sim 13\times$ su un'equazione di Burgers 1D, suggerendo un forte potenziale per la modellazione di singolarità fisiche.
\end{abstract}

\tableofcontents
\vspace{1em}

% ============================================================
\section{Formulazione Matematica e Derivazioni}

NOVA è definita come la composizione di un meccanismo di gating esponenziale e una regolarizzazione razionale sottrattiva:
\begin{equation}\label{eq:nova}
f(x) = x \cdot \sigma(\beta x) - \frac{x}{1 + (\beta x)^2}
\end{equation}
dove $\sigma(z) = (1 + e^{-z})^{-1}$ è la funzione logistica standard e $\beta$ è un parametro continuo che modula la curvatura.

\subsection{Analisi del Gradiente e Comportamento Asintotico}

La derivata prima $f'(x)$, che governa il flusso del gradiente durante la retropropagazione, è data analiticamente da:
\begin{equation}\label{eq:deriv1}
f'(x) = \sigma(\beta x) + \beta x \cdot \sigma(\beta x)\bigl(1 - \sigma(\beta x)\bigr)
        - \frac{1 - (\beta x)^2}{\bigl(1 + (\beta x)^2\bigr)^2}
\end{equation}

Per valutare il comportamento in prossimità dell'origine (assumendo $\beta = 1$), calcoliamo $f'(0)$ passo per passo:
\begin{align*}
f'(0) &= \sigma(0) + 0 \cdot \sigma(0)\bigl(1 - \sigma(0)\bigr) - \frac{1 - 0^2}{\bigl(1 + 0^2\bigr)^2} \\
      &= \frac{1}{2} + 0 - \frac{1}{1} \\
      &= -0.5
\end{align*}

Questo gradiente marcatamente negativo all'origine definisce la \textit{sacca di non-monotonicità} di NOVA. A differenza di GELU o SiLU, dove la concavità negativa è debole, NOVA esercita una forte spinta repulsiva (hard-thresholding implicito) per attivazioni prossime allo zero. Asintoticamente, per $x \to +\infty$, il termine razionale decade a zero ($O(1/x)$) e $\sigma(x) \to 1$, restituendo $f(x) \approx x$. Per $x \to -\infty$, $f(x) \to 0$.

\subsection{Conservazione della Varianza (NOVA-Init)}

Il mantenimento dell'entropia del segnale nei layer profondi richiede una corretta inizializzazione dei pesi. Nel framework generalizzato di He et al.~\cite{he2015delving}, la varianza ideale è:
\[
\mathrm{Var}(W) = \frac{1}{n_\text{in} \cdot \mathbb{E}[f(X)^2]}, \quad X \sim \mathcal{N}(0,1)
\]

A differenza di ReLU, per cui $\mathbb{E}[\max(0,X)^2] = 0.5$ (regola di He standard $2/n_\text{in}$), NOVA possiede una regione contrattiva. Integrando numericamente l'aspettativa dell'attivazione al quadrato sotto distribuzione normale:
\begin{equation}
\mathbb{E}[f(X)^2] = \int_{-\infty}^{+\infty}
    \!\left(x\sigma(x) - \frac{x}{1+x^2}\right)^{\!2}
    \frac{e^{-x^2/2}}{\sqrt{2\pi}}\,dx \approx 0.357
\end{equation}

Pertanto, per prevenire il \textit{signal decay} nelle prime fasi del training:
\begin{equation}
\mathrm{Var}(W_\text{NOVA}) = \frac{1}{n_\text{in} \cdot 0.357} \approx \frac{2.801}{n_\text{in}}
\end{equation}

\subsection{Derivata Seconda ed Espressività per le PINN}\label{sec:deriv2}

Scomponendo $f(x) = S(x) - R(x)$, dove $S(x) = x\sigma(\beta x)$ è il termine SiLU e $R(x) = x/(1+(\beta x)^2)$ è il termine razionale, deriviamo l'Hessiana 1D di NOVA:
\begin{equation}\label{eq:deriv2}
f''(x) =
\underbrace{2\beta\sigma'(\beta x) + \beta^2 x\,\sigma'(\beta x)\bigl(1 - 2\sigma(\beta x)\bigr)}_{S''(x)}
-
\underbrace{\frac{2\beta^2 x\bigl(\beta^2 x^2 - 3\bigr)}{\bigl(1 + \beta^2 x^2\bigr)^3}}_{R''(x)}
\end{equation}

La derivata seconda di funzioni esponenziali pure (come GELU) decade rapidamente per $|x| > 2$. In NOVA, il termine $R''(x)$ introduce una coda polinomiale (\textit{heavy-tailed}), che permette all'operatore Laplaciano $u_{xx}$ calcolato dalla rete di catturare variazioni ad alta frequenza (fronti d'urto, discontinuità termiche) che funzioni troppo lisce tendono a sfumare.

% ============================================================
\section{Contesto Architetturale e Stato dell'Arte}

\subsection{Confronto con Funzioni Alternative}

NOVA si colloca in un ecosistema dove si cerca il bilanciamento tra levigatezza (\textit{smoothness}) e capacità di thresholding. La Tabella~\ref{tab:comparison} riassume le differenze strutturali con le principali funzioni di attivazione.

\begin{table}[h]
\centering
\caption{Confronto tassonomico preliminare delle principali funzioni di attivazione.}
\label{tab:comparison}
\begin{tabular}{lllllc}
\toprule
\textbf{Funzione} & \textbf{Formula} & \textbf{Monotona} & \textbf{$C^\infty$} & \textbf{Coda $f''$} & \textbf{Costo Rel.} \\
\midrule
ReLU      & $\max(0,x)$      & Sì & No  & Nulla       & $1.0\times$ \\
GELU \cite{hendrycks2016gaussian} & $x\Phi(x)$ & No & Sì & Esponenziale & $2.5\times$ \\
SiLU/Swish \cite{ramachandran2017searching} & $x\sigma(x)$ & No & Sì & Esponenziale & $2.2\times$ \\
Mish \cite{misra2019mish}      & $x\tanh(\mathrm{softplus}(x))$ & No & Sì & Esponenziale & $2.4\times$ \\
Snake \cite{ziyin2020neural}   & $x + \sin^2(x)$  & No & Sì & Periodica   & $4.0\times$ \\
NOVA      & Eq.~\eqref{eq:nova} & No & Sì & Polinomiale & $2.8\times$ \\
\bottomrule
\end{tabular}
\end{table}

Rispetto a Snake (ideale per segnali periodici, ma instabile nei Transformer) e SiLU (eccellente per LLM ma debole su operatori differenziali), NOVA fornisce asintoti globali stabili con perturbazioni locali heavy-tailed.

\subsection{Teorema della Covariance Shift Collision}

Nel nostro studio esplorativo su Convolutional Neural Networks (CNN), abbiamo riscontrato un fallimento critico di NOVA abbinata alla Batch Normalization (BN). Un layer BN standardizza le pre-attivazioni $X$ imponendo $\mu = 0$ e $\sigma^2 = 1$: circa il $68\%$ della massa di probabilità ricade nell'intervallo $x \in [-1, 1]$.

In NOVA, questa regione coincide quasi esattamente con il dominio in cui $f'(x) \leq 0$, raggiungendo il minimo in $f'(0) = -0.5$. Poiché l'entropia differenziale dell'output $h(Y)$ con $Y = f(X)$ è limitata da $\mathbb{E}[\log|f'(X)|]$, forzare la maggior parte del segnale dove la derivata è contrattiva e non-monotona causa un drastico collasso della Mutua Informazione $I(X; Y)$ tra layer adiacenti. Al contrario, la Layer Normalization (usata nei Transformer) non impone una centratura globale, permettendo ai vettori di feature di mantenere una covarianza più naturale e di sfruttare la sacca negativa di NOVA come meccanismo selettivo per il rumore a bassa magnitudine.

\subsection{Ablation Study Analitico}

Dal punto di vista teorico, il comportamento di NOVA può essere disaccoppiato in due meccanismi:

\begin{itemize}
  \item \textbf{Termine esponenziale $x\sigma(\beta x)$ (Gating):} Fornisce il limite asintotico lineare e garantisce la stabilità del gradiente per attivazioni fortemente positive. Senza questo componente, la limitatezza asintotica del termine razionale causerebbe la divergenza della rete.
  \item \textbf{Termine razionale $x/(1+(\beta x)^2)$ (Damping):} Modula e approfondisce la sacca non-monotona ed è l'unico responsabile della coda heavy-tailed nella derivata seconda (Eq.~\eqref{eq:deriv2}), senza la quale le performance su operatori differenziali regredirebbero a quelle di SiLU.
\end{itemize}

% ============================================================
\section{Validazione Empirica Preliminare}

Tutti gli esperimenti riportati in questa sezione sono \textbf{indagini a run singola (singolo seed)}, condotte per valutare trend di convergenza e formare ipotesi preliminari. Studi futuri con replicazione statistica saranno necessari per trarre conclusioni definitive.

\subsection{Classificazione Visiva (Mini-ViT su CIFAR-100)}

Abbiamo addestrato da zero un Mini-ViT (4 layer, embed\_dim=256, 4 heads, MLP$\times$4) su CIFAR-100 per 100 epoche con Data Augmentation, Label Smoothing (0.1), Mixed Precision (FP16) e LayerNorm. L'addestramento è stato effettuato con AdamW (lr=$3\!\times\!10^{-3}$, weight\_decay=$0.05$), warmup lineare (10 epoche) seguito da cosine annealing, su 2$\times$ GPU NVIDIA T4 (Kaggle). I risultati (Tabella~\ref{tab:vit}) confrontano cinque funzioni di attivazione.

NOVA raggiunge una Best Test Accuracy del $56.75\%$, superando GELU di $+2.08$ punti percentuali. Il margine è consistente anche rispetto a ReLU, SiLU e Mish. Pur essendo una run singola, un divario di oltre 2 punti su CIFAR-100 a 100 epoche suggerisce che il meccanismo di auto-regolarizzazione geometrica di NOVA fornisca un vantaggio qualitativo in architetture LayerNorm-based rispetto alle attivazioni puramente esponenziali.

\begin{table}[h]
\centering
\caption{Best Test Accuracy -- Mini-ViT su CIFAR-100 (run singola, seed 42, 100 epoche, FP16).}
\label{tab:vit}
\begin{tabular}{lcc}
\toprule
\textbf{Attivazione} & \textbf{Best Val Acc} & \textbf{$\Delta$ vs GELU} \\
\midrule
\textbf{NOVA} & \textbf{56.75\%} & \textbf{+2.08} \\
GELU           & 54.67\%            & ---            \\
ReLU           & 54.55\%            & $-0.12$        \\
SiLU           & 53.84\%            & $-0.83$        \\
Mish           & 53.25\%            & $-1.42$        \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Studio di Scaling (ViT su CIFAR-100)}

Per verificare se il vantaggio di NOVA si mantenga all'aumentare della capacità del modello, abbiamo condotto uno studio di scaling con tre configurazioni di ViT di dimensione crescente, confrontando NOVA e GELU a 100 epoche su CIFAR-100. Ogni scala utilizza iperparametri calibrati per le GPU T4 (Tabella~\ref{tab:scaling}).

\begin{table}[h]
\centering
\caption{Scaling study -- ViT su CIFAR-100 (NOVA vs GELU, run singola, seed 42, 100 epoche, FP16, 2$\times$T4).}
\label{tab:scaling}
\begin{tabular}{llcccc}
\toprule
\textbf{Scala} & \textbf{Config} & \textbf{Params} & \textbf{NOVA} & \textbf{GELU} & \textbf{$\Delta$} \\
\midrule
Tiny  & 4L, 256d, 4h & 3.2M  & \textbf{56.71\%} & 54.43\% & +2.28 \\
Small & 6L, 384d, 6h & 10.7M & \textbf{59.37\%} & 55.48\% & +3.89 \\
Base  & 8L, 512d, 8h & 25.3M & \textbf{55.22\%} & 51.15\% & +4.07 \\
\bottomrule
\end{tabular}
\end{table}

NOVA supera GELU in tutte e tre le scale, con un divario che cresce da $+2.28$ (Tiny) a $+4.07$ punti percentuali (Base). L'osservazione suggerisce che il meccanismo di auto-regolarizzazione geometrica di NOVA fornisca un vantaggio crescente con la capacità del modello.

Tuttavia, i risultati rivelano un fenomeno di \textbf{overfitting marcato} a tutte le scale. La Tabella~\ref{tab:overfit} riporta il divario tra training accuracy e validation accuracy a fine addestramento.

\begin{table}[h]
\centering
\caption{Overfitting gap (Train Acc $-$ Val Acc) a fine addestramento -- ViT Scaling.}
\label{tab:overfit}
\begin{tabular}{lcccccc}
\toprule
 & \multicolumn{3}{c}{\textbf{NOVA}} & \multicolumn{3}{c}{\textbf{GELU}} \\
\cmidrule(lr){2-4}\cmidrule(lr){5-7}
\textbf{Scala} & \textbf{Train} & \textbf{Val} & \textbf{Gap} & \textbf{Train} & \textbf{Val} & \textbf{Gap} \\
\midrule
Tiny  & 98.55\% & 56.24\% & 42.31 & 96.69\% & 54.41\% & 42.28 \\
Small & 99.93\% & 59.33\% & 40.60 & 99.80\% & 55.41\% & 44.39 \\
Base  & 99.96\% & 55.21\% & 44.75 & 99.86\% & 51.03\% & 48.83 \\
\bottomrule
\end{tabular}
\end{table}

Alla scala Base, l'accuracy in validazione di entrambe le attivazioni scende al di sotto della scala Small (NOVA: $55.22\%$ vs $59.37\%$; GELU: $51.15\%$ vs $55.48\%$), indicando che l'aumento di capacità non viene sfruttato per generalizzare, ma per memorizzare il training set. Osserviamo che il gap di overfitting è sistematicamente inferiore per NOVA rispetto a GELU (specialmente a scala Small e Base), il che suggerisce un effetto di regolarizzazione implicita coerente con la struttura contrattiva della sacca non-monotona. Tuttavia, questo effetto non è sufficiente a prevenire il degrado prestazionale al crescere del modello su un dataset di dimensione limitata come CIFAR-100, motivando l'indagine successiva con tecniche di regolarizzazione avanzate.

\subsection{Scaling Study con Regolarizzazione DeiT-style (v2)}

Per affrontare l'overfitting marcato osservato nello studio di scaling v1, abbiamo ripetuto l'esperimento applicando tre tecniche di regolarizzazione ispirate a DeiT~\cite{touvron2021training}: (1) \textbf{RandAugment}~\cite{cubuk2020randaugment} (2 operazioni, magnitudine 9); (2) \textbf{CutMix}~\cite{yun2019cutmix} ($\alpha\!=\!1.0$) + \textbf{Mixup}~\cite{zhang2018mixup} ($\alpha\!=\!0.8$) con switch probability $0.5$; (3) \textbf{Stochastic Depth}~\cite{huang2016deep} (DropPath) con tasso crescente per scala (Tiny: $0.1$, Small: $0.2$, Base: $0.3$). Tutti gli altri iperparametri sono invariati rispetto a v1.

\begin{table}[h]
\centering
\caption{Scaling study v2 (con regolarizzazione) -- ViT su CIFAR-100 (NOVA vs GELU, run singola, seed 42, 100 epoche, FP16, 2$\times$T4).}
\label{tab:scaling_v2}
\begin{tabular}{llccccc}
\toprule
\textbf{Scala} & \textbf{Params} & \textbf{NOVA v2} & \textbf{GELU v2} & \textbf{$\Delta$} & \textbf{NOVA v1} & \textbf{$\Delta$ v2$-$v1} \\
\midrule
Tiny  & 3.2M  & 51.72\% & 45.12\% & +6.60 & 56.71\% & $-4.99$ \\
Small & 10.7M & \textbf{62.66\%} & 54.31\% & +8.35 & 59.37\% & $+3.29$ \\
Base  & 25.3M & 60.31\% & 53.14\% & +7.17 & 55.22\% & $+5.09$ \\
\bottomrule
\end{tabular}
\end{table}

I risultati (Tabella~\ref{tab:scaling_v2}) rivelano tre fenomeni rilevanti:

\textbf{(1) Recupero della scala Base.} Con la regolarizzazione v2, NOVA-Base raggiunge il $60.31\%$, un miglioramento di $+5.09$ punti percentuali rispetto a v1 ($55.22\%$). Il modello ora sfrutta la capacità aggiuntiva per generalizzare anziché memorizzare. Analogamente, GELU-Base migliora da $51.15\%$ a $53.14\%$ ($+1.99$).

\textbf{(2) Sotto-adattamento alla scala Tiny.} La combinazione di RandAugment, CutMix/Mixup e DropPath risulta eccessivamente aggressiva per il modello più piccolo (3.2M parametri), causando una riduzione dell'accuracy: NOVA scende da $56.71\%$ a $51.72\%$ ($-4.99$), GELU da $54.43\%$ a $45.12\%$ ($-9.31$). L'osservazione suggerisce che la forza della regolarizzazione debba essere calibrata rispetto alla capacità del modello.

\textbf{(3) Ampliamento del vantaggio di NOVA.} Con regolarizzazione adeguata, il divario NOVA--GELU si amplifica significativamente: da $+2.28$/$+3.89$/$+4.07$ (v1) a $+6.60$/$+8.35$/$+7.17$ punti percentuali (v2). In particolare, alla scala Small NOVA raggiunge il $62.66\%$, il miglior risultato assoluto osservato in tutti i nostri esperimenti ViT su CIFAR-100. Questo suggerisce che la struttura auto-regolarizzante di NOVA sia complementare alla regolarizzazione esplicita: mentre CutMix/Mixup/DropPath prevengono la memorizzazione dei dati, la sacca non-monotona di NOVA sembra fornire un bias induttivo geometrico aggiuntivo che GELU non possiede.

\begin{table}[h]
\centering
\caption{Confronto overfitting gap (Train Acc $-$ Val Acc) -- v1 vs v2. \textit{Nota:} in v2, CutMix/Mixup rendono le immagini di training miste con label soft, producendo un'accuracy di training non confrontabile direttamente con la validazione.}
\label{tab:overfit_v2}
\begin{tabular}{lcccc}
\toprule
 & \multicolumn{2}{c}{\textbf{NOVA}} & \multicolumn{2}{c}{\textbf{GELU}} \\
\cmidrule(lr){2-3}\cmidrule(lr){4-5}
\textbf{Scala} & \textbf{Gap v1} & \textbf{Gap v2} & \textbf{Gap v1} & \textbf{Gap v2} \\
\midrule
Tiny  & 42.31 & $-22.23$ & 42.28 & $-20.75$ \\
Small & 40.60 & $-16.92$ & 44.39 & $-18.88$ \\
Base  & 44.75 & $-14.36$ & 48.83 & $-14.90$ \\
\bottomrule
\end{tabular}
\end{table}

La Tabella~\ref{tab:overfit_v2} mostra l'inversione completa del gap di overfitting: in v2 il training accuracy è sistematicamente inferiore al validation accuracy. Questo è un artefatto atteso di CutMix/Mixup, che creano immagini miste con label distribuzionali durante il training, rendendo il compito di training intrinsecamente più difficile della valutazione su immagini pulite. L'overfitting strutturale osservato in v1 (gap $>40$ pp) è stato completamente eliminato.

\subsubsection{Evoluzione del Parametro $\beta$}

Il parametro apprendibile $\beta$ di NOVA mostra una dinamica di convergenza coerente in v2: partendo da $\beta_0 = 1.0$, decresce rapidamente nelle prime 20 epoche, stabilizzandosi attorno a $\beta \approx 0.45$ per tutte e tre le scale (Tiny: $0.478$, Small: $0.444$, Base: $0.448$). Questo comportamento suggerisce che l'ottimizzazione comprime spontaneamente l'ampiezza della sacca non-monotona per adattarsi al regime regolarizzato, riducendo la curvatura della funzione di attivazione quando la regolarizzazione esterna è già presente.

\subsection{Validazione Cross-Dataset (ViT su Tiny-ImageNet-200)}

Per verificare che il vantaggio di NOVA non sia specifico a CIFAR-100, abbiamo replicato lo studio di scaling su Tiny-ImageNet-200 (200 classi, 100K immagini di training, 10K di validazione, risoluzione $64\times64$). L'architettura ViT è la stessa famiglia a tre scale (Tiny/Small/Base), con \texttt{patch\_size}$\,=8$ ($\to 64$ patch per immagine). La regolarizzazione è calibrata per scala: Tiny con RandAugment magnitudine~5, CutMix/Mixup probabilità~$0.5$, DropPath~$0.05$; Small e Base con regolarizzazione DeiT-style piena (magnitudine~9, probabilità~$1.0$, DropPath~$0.2$/$0.3$). L'addestramento è stato condotto per 100 epoche con AdamW su 2$\times$T4 in FP16.

\begin{table}[h]
\centering
\caption{ViT su Tiny-ImageNet-200 -- NOVA vs GELU (run singola, seed 42, 100 epoche, FP16, 2$\times$T4).}
\label{tab:tinyimagenet}
\begin{tabular}{llccccc}
\toprule
\textbf{Scala} & \textbf{Params} & \textbf{NOVA Top-1} & \textbf{GELU Top-1} & \textbf{$\Delta$} & \textbf{NOVA Top-5} & \textbf{GELU Top-5} \\
\midrule
Tiny  & 3.3M  & 47.71\% & 45.96\% & +1.75 & 72.52\% & 71.32\% \\
Small & 10.8M & \textbf{51.01\%} & 46.43\% & +4.58 & \textbf{75.49\%} & 72.18\% \\
Base  & 25.5M & 50.75\% & 44.70\% & +6.05 & 75.32\% & 70.15\% \\
\bottomrule
\end{tabular}
\end{table}

I risultati (Tabella~\ref{tab:tinyimagenet}) confermano il pattern osservato su CIFAR-100: il vantaggio di NOVA cresce monotonamente con la scala del modello, da $+1.75$ a $+6.05$ punti percentuali in Top-1 accuracy. L'accuracy Top-5 mostra lo stesso trend ($+1.20$ a $+5.17$ pp).

\begin{table}[h]
\centering
\caption{Confronto cross-dataset del vantaggio NOVA--GELU (pp, Top-1 accuracy).}
\label{tab:cross_dataset}
\begin{tabular}{lcc}
\toprule
\textbf{Scala} & \textbf{CIFAR-100 (v2)} & \textbf{Tiny-ImageNet-200} \\
\midrule
Tiny  & +6.60 & +1.75 \\
Small & +8.35 & +4.58 \\
Base  & +7.17 & +6.05 \\
\bottomrule
\end{tabular}
\end{table}

Il vantaggio assoluto è più contenuto su Tiny-ImageNet-200 (Tabella~\ref{tab:cross_dataset}), il che è atteso: il dataset è più complesso (200 classi vs 100, risoluzione $64\times64$ vs $32\times32$) e i batch size ridotti (512/256/128 vs 1024/512/256) potrebbero aver limitato la convergenza. Tuttavia, il trend \textit{monotonamente crescente} del vantaggio con la scala è preservato su entrambi i dataset, rafforzando l'ipotesi che la regolarizzazione geometrica intrinseca di NOVA diventi più efficace al crescere della capacità del modello.

Il parametro $\beta$ converge a valori leggermente superiori rispetto a CIFAR-100 v2 ($\approx 0.54$--$0.58$ vs $\approx 0.45$), suggerendo un adattamento automatico alla maggiore complessità del dataset.

\subsection{Generalizzazione Architetturale (ConvNeXt su CIFAR-100)}

Per verificare che il vantaggio di NOVA non sia specifico ai Vision Transformer, abbiamo condotto uno studio di scaling su ConvNeXt~\cite{liu2022convnet}, un'architettura interamente convolutiva (no attention) che adotta GELU per design. La sostituzione di GELU con NOVA è quindi un test diretto e pulito dell'impatto della funzione di attivazione in contesto CNN.

Abbiamo adattato ConvNeXt per CIFAR-100 ($32\times32$) con stem stride-1, 4 stage con downsample $32\to16\to8\to4$, Layer Scale ($\gamma_0=10^{-6}$), e tre scale calibrate per avere parametri comparabili ai ViT: Tiny (dims $[40, 80, 160, 320]$, depths $[2,2,6,2]$, 3.5M), Small (dims $[64, 128, 256, 512]$, depths $[3,3,6,3]$, 11.0M), Base (dims $[96, 192, 384, 768]$, depths $[3,3,9,3]$, 28.1M). La regolarizzazione segue lo stesso schema calibrato per scala: Tiny con RandAugment magnitudine~5, CutMix/Mixup probabilità~$0.5$, DropPath~$0.05$; Small e Base con regolarizzazione piena.

\begin{table}[h]
\centering
\caption{ConvNeXt su CIFAR-100 -- NOVA vs GELU (run singola, seed 42, 100 epoche, FP16, 2$\times$T4).}
\label{tab:convnext}
\begin{tabular}{llcccc}
\toprule
\textbf{Scala} & \textbf{Config} & \textbf{Params} & \textbf{NOVA} & \textbf{GELU} & \textbf{$\Delta$} \\
\midrule
Tiny  & $[40,80,160,320]$, $[2,2,6,2]$ & 3.5M  & 65.88\% & 65.74\% & +0.14 \\
Small & $[64,128,256,512]$, $[3,3,6,3]$ & 11.0M & 75.01\% & 74.36\% & +0.65 \\
Base  & $[96,192,384,768]$, $[3,3,9,3]$ & 28.1M & \textbf{79.03\%} & 78.07\% & +0.96 \\
\bottomrule
\end{tabular}
\end{table}

I risultati (Tabella~\ref{tab:convnext}) confermano che NOVA supera GELU a tutte le scale anche in un'architettura puramente convolutiva, con un vantaggio che cresce monotonamente con la capacità ($+0.14 \to +0.65 \to +0.96$ pp). Tuttavia, i margini sono significativamente inferiori rispetto al ViT.

\subsubsection{Sensibilità agli Iperparametri (v2)}

Per verificare la robustezza dei risultati, abbiamo ripetuto l'esperimento con batch size e learning rate ridotti: Tiny e Small con batch~256, lr=$1\!\times\!10^{-3}$; Base con batch~128, lr=$5\!\times\!10^{-4}$.

\begin{table}[h]
\centering
\caption{ConvNeXt su CIFAR-100 v2 (iperparametri ottimizzati) -- NOVA vs GELU (run singola, seed 42, 100 epoche, FP16, 2$\times$T4).}
\label{tab:convnext_v2}
\begin{tabular}{llccccc}
\toprule
\textbf{Scala} & \textbf{Params} & \textbf{NOVA v2} & \textbf{GELU v2} & \textbf{$\Delta$} & \textbf{NOVA v1} & \textbf{$\Delta$ v2$-$v1} \\
\midrule
Tiny  & 3.5M  & \textbf{70.97\%} & 69.45\% & +1.52 & 65.88\% & $+5.09$ \\
Small & 11.0M & 76.39\% & 75.46\% & +0.93 & 75.01\% & $+1.38$ \\
Base  & 28.1M & 77.99\% & 76.62\% & +1.37 & 79.03\% & $-1.04$ \\
\bottomrule
\end{tabular}
\end{table}

I risultati (Tabella~\ref{tab:convnext_v2}) rivelano due fenomeni rilevanti. (1) Il vantaggio NOVA--GELU si amplifica significativamente con iperparametri più conservativi: da $+0.14$/$+0.65$/$+0.96$ (v1) a $+1.52$/$+0.93$/$+1.37$ pp (v2). In particolare, alla scala Tiny il delta cresce di un fattore $\sim\!10\times$. (2) L'accuracy assoluta mostra una sensibilità scala-dipendente al batch size: Tiny migliora di $+5$ pp (sotto-adattamento con batch troppo grandi in v1), mentre Base perde $\sim\!1$ pp (il batch 512 di v1 era più efficace per il modello più grande). L'osservazione è analoga al pattern di regolarizzazione calibrata già riscontrato nei ViT.

\begin{table}[h]
\centering
\caption{Confronto cross-architettura del vantaggio NOVA--GELU (pp) su CIFAR-100.}
\label{tab:arch_comparison}
\begin{tabular}{lccc}
\toprule
\textbf{Scala} & \textbf{ViT (v2)} & \textbf{ConvNeXt v1} & \textbf{ConvNeXt v2} \\
\midrule
Tiny  & +6.60 & +0.14 & +1.52 \\
Small & +8.35 & +0.65 & +0.93 \\
Base  & +7.17 & +0.96 & +1.37 \\
\bottomrule
\end{tabular}
\end{table}

La differenza di magnitudine tra ViT e ConvNeXt (Tabella~\ref{tab:arch_comparison}) persiste anche con iperparametri ottimizzati, confermando che NOVA è particolarmente efficace nelle architetture dove l'attivazione è l'unica fonte di non-linearità tra proiezioni lineari dense (come nel MLP dei Transformer), mentre in ConvNeXt le convoluzioni depthwise forniscono già un forte bias induttivo spaziale che riduce la dipendenza dalla funzione di attivazione. Il parametro $\beta$ conferma questa interpretazione: in ConvNeXt, $\beta$ converge a valori significativamente più bassi ($\approx 0.23$--$0.30$ vs $\approx 0.45$ nel ViT), riducendo l'ampiezza della sacca non-monotona e avvicinando il comportamento di NOVA a quello di SiLU. L'ottimizzazione adatta spontaneamente la curvatura al contesto architetturale.

\subsection{Modellazione Autoregressiva (Nano-GPT)}

Abbiamo addestrato un Nano-GPT ($\approx$10M parametri) sul dataset TinyShakespeare per 1000 iterazioni. NOVA ha prodotto una Validation Loss di $1.6949$ rispetto a $1.7344$ di GELU. L'osservazione suggerisce che il meccanismo di gating potenziato di NOVA possa contribuire a mitigare la saturazione dimensionale nei blocchi Feed-Forward dei Transformer.

\begin{table}[h]
\centering
\caption{Validation Loss (Cross-Entropy) -- Nano-GPT su TinyShakespeare (run singola).}
\label{tab:gpt}
\begin{tabular}{lcc}
\toprule
\textbf{Step} & \textbf{NOVA} & \textbf{GELU} \\
\midrule
0    & 4.3083 & 4.3501 \\
400  & 1.9542 & 1.9747 \\
1000 & 1.6949 & 1.7344 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Modelli Generativi a Diffusione (DDPM)}

\subsubsection{U-Net DDPM (Fashion-MNIST)}

Su una U-Net DDPM per il denoising di immagini Fashion-MNIST, dopo 10 epoche GELU ha ottenuto una MSE Loss di $0.0372$ contro $0.0382$ di NOVA. Abbiamo ipotizzato uno \textit{Smoothness Trade-off}: la U-Net usa Batch Normalization, che entra in conflitto con la non-monotonia di NOVA (Sezione~2.3). Per verificare questa ipotesi, abbiamo progettato un esperimento su Diffusion Transformer (DiT), che usa LayerNorm + AdaLN senza BatchNorm.

\subsubsection{DiT DDPM (CIFAR-10)}

Per testare se il risultato negativo su U-Net fosse attribuibile alla Covariance Shift Collision con BatchNorm piuttosto che a un limite intrinseco di NOVA nel contesto generativo, abbiamo implementato un Diffusion Transformer (DiT)~\cite{peebles2023scalable} su CIFAR-10 ($32\times32$, 10 classi). Il DiT usa LayerNorm + AdaLN-Zero per il condizionamento temporale, esattamente il contesto normativo dove NOVA eccelle nei ViT classificativi.

\textbf{Architettura.} DiT-Base (8 layer, embed\_dim=384, 6 heads, MLP$\times$4, $\approx$22M parametri), con patchify $4\times4$ ($\to 64$ patch), positional embedding apprendibile, e condizionamento timestep sinusoidale $\to$ Linear $\to$ SiLU $\to$ Linear (SiLU fisso nell'embedding, attivazione variabile solo nel backbone MLP). Cosine noise schedule~\cite{nichol2021improved} con $T=1000$.

\textbf{Training.} AdamW (lr=$3\!\times\!10^{-4}$, weight\_decay=$0.01$), warmup 10 epoche + cosine decay, 400 epoche totali, FP16+GradScaler, EMA decay $0.9995$, gradient clipping $1.0$, batch size 128, su GPU NVIDIA T4. Augmentation: RandomHorizontalFlip + RandomCrop($32$, padding=$4$).

\textbf{Valutazione.} FID e IS su 10K campioni generati con DDIM~\cite{song2020denoising} (250 step, $\eta=0$) ogni 25 epoche, usando features Inception v3 (pool layer, 2048-dim). Val loss con EMA model.

\begin{table}[h]
\centering
\caption{DiT-Base DDPM su CIFAR-10 -- NOVA vs GELU (run singola, seed 42, 400 epoche, FP16, T4).}
\label{tab:dit}
\begin{tabular}{lcccc}
\toprule
\textbf{Attivazione} & \textbf{Best Val Loss} & \textbf{Best FID $\downarrow$} & \textbf{Best IS $\uparrow$} & \textbf{$\beta$ finale} \\
\midrule
NOVA & 0.054673 & 21.74 & 4.10 & 0.756 \\
GELU & 0.054585 & 21.45 & 4.11 & --- \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Evoluzione del FID durante il training -- DiT-Base DDPM su CIFAR-10 (DDIM 250 step).}
\label{tab:dit_fid_evolution}
\begin{tabular}{lcccccc}
\toprule
\textbf{Epoca} & \textbf{25} & \textbf{100} & \textbf{200} & \textbf{300} & \textbf{375} & \textbf{400} \\
\midrule
NOVA & 88.05 & 32.23 & 25.30 & 22.68 & \textbf{21.74} & 22.06 \\
GELU & 86.20 & 31.48 & 25.19 & 22.25 & 21.56 & \textbf{21.45} \\
\bottomrule
\end{tabular}
\end{table}

I risultati (Tabelle~\ref{tab:dit} e~\ref{tab:dit_fid_evolution}) mostrano che NOVA e GELU sono \textbf{sostanzialmente equivalenti} su DiT-Base: il divario in FID ($0.29$ punti), val loss ($0.000088$) e IS ($0.01$) è trascurabile per una run singola. Le curve di FID sono quasi sovrapposte per tutta la durata del training (400 epoche), con entrambe le attivazioni che convergono a FID $\approx 21$--$22$ e IS $\approx 4.1$.

Il parametro $\beta$ di NOVA converge a $\approx 0.756$, un valore notevolmente più alto rispetto ai ViT classificativi ($\approx 0.45$) e ai ConvNeXt ($\approx 0.25$). Questo suggerisce che nel contesto generativo l'ottimizzazione mantiene una sacca non-monotona più ampia, ma il suo effetto netto sulle metriche è neutro.

\textbf{Interpretazione.} Il risultato DiT ridimensiona l'ipotesi originale. L'eliminazione della BatchNorm (passando dalla U-Net al DiT) ha rimosso il degrado prestazionale di NOVA: NOVA non è più peggiore di GELU. Tuttavia, non si osserva il vantaggio significativo riscontrato nei ViT classificativi ($+6$--$8$ pp). Questo suggerisce che la sacca non-monotona di NOVA sia \textit{neutra} nel contesto $\varepsilon$-prediction: non introduce le perturbazioni dannose viste con BatchNorm, ma il suo meccanismo di regolarizzazione geometrica non fornisce benefici misurabili quando l'obiettivo è la ricostruzione di rumore anziché la classificazione discriminativa.

\subsection{Scientific Machine Learning: PINN sull'Equazione di Burgers}

Il test qualitativamente più rilevante ha riguardato una PINN addestrata a risolvere l'equazione non-lineare di Burgers 1D, che modella la formazione di onde d'urto fluidodinamiche e richiede il calcolo dell'operatore Laplaciano tramite auto-differenziazione doppia. Dopo 2000 step di ottimizzazione, la Physics Loss (residuo PDE) è stata:

\begin{table}[h]
\centering
\caption{Physics Loss (residuo PDE) -- PINN sull'Equazione di Burgers 1D (run singola).}
\label{tab:pinn}
\begin{tabular}{lcc}
\toprule
\textbf{Epoca} & \textbf{NOVA} & \textbf{GELU} \\
\midrule
200  & 0.07808 & 0.09757 \\
1000 & 0.00207 & 0.00842 \\
2000 & 0.00027 & 0.00353 \\
\bottomrule
\end{tabular}
\end{table}

Nonostante l'assenza di replicazione statistica, un abbattimento del residuo di oltre un ordine di grandezza (fattore $\sim 13\times$) suggerisce fortemente che la struttura polinomiale heavy-tailed della derivata seconda di NOVA (Eq.~\eqref{eq:deriv2}) sia geometricamente superiore alle code esponenziali per approssimare singolarità e variazioni brusche tipiche delle onde d'urto. Questo risultato è coerente con la letteratura sulle attivazioni non-smooth per SciML~\cite{raissi2019physics}.

% ============================================================
\section{Efficienza Computazionale (CUDA Fusion)}

A causa della sua complessità composita, NOVA in modalità \textit{Eager} su PyTorch richiede l'allocazione ripetuta di tensori temporanei intermedi, generando un Memory Bandwidth Bottleneck. Abbiamo sviluppato un \textbf{Fused CUDA Kernel} che sfrutta le istruzioni hardware \textit{Fast Math} (\texttt{\_\_expf}, \texttt{\_\_fdividef}) direttamente nei registri del multiprocessore.

Un benchmark esplorativo su GPU NVIDIA T4 (tensore FP32 $2048 \times 2048$, 100 iterazioni) ha prodotto i seguenti tempi per un ciclo Forward+Backward:

\begin{table}[h]
\centering
\caption{Latenza Forward+Backward -- GPU NVIDIA T4, FP32, tensore $2048\times2048$.}
\label{tab:cuda}
\begin{tabular}{llcc}
\toprule
\textbf{Funzione} & \textbf{Implementazione} & \textbf{Latency} & \textbf{VRAM Intermedia} \\
\midrule
NOVA & Python Eager         & $\approx 4.56$ ms & 128 MB \\
GELU & ATen Native Fused    & $\approx 0.57$ ms & 0 MB   \\
NOVA & Custom CUDA Fused    & $\approx 0.92$ ms & 0 MB   \\
\bottomrule
\end{tabular}
\end{table}

Il kernel fused azzera il memory footprint intermedio e chiude circa il $90\%$ del gap di latenza rispetto a GELU, rendendo NOVA sufficientemente efficiente per il training distribuito. L'overhead residuo ($\sim 0.35$ ms) è attribuibile ai cicli aggiuntivi richiesti dalla divisione razionale rispetto alle sole operazioni esponenziali.

% ============================================================
\section{Limitazioni e Lavori Futuri}

Il presente lavoro costituisce una \textit{Proof of Concept} e presenta le seguenti limitazioni da affrontare in studi successivi:

\begin{itemize}
  \item \textbf{Robustezza Statistica:} Tutti i risultati sperimentali derivano da run singole con singolo seed. La replicazione con seed multipli e test statistici formali è necessaria per validare i trend osservati.
  \item \textbf{Scala e Generalizzazione:} Lo studio di scaling v2 con regolarizzazione DeiT-style ha eliminato l'overfitting e la validazione cross-dataset su Tiny-ImageNet-200 ha confermato il trend, ma tutti gli esperimenti sono limitati a $\leq$25M parametri su dataset di dimensione modesta. Il comportamento di NOVA a scale LLM ($\geq$7B parametri) e su dataset su larga scala (ImageNet-1K) rimane non verificato.
  \item \textbf{Stabilità di $\beta$:} Nello studio v2, il parametro $\beta$ converge stabilmente a $\approx 0.45$ partendo da $\beta_0 = 1.0$, ma la sensibilità all'inizializzazione $\beta_0$ e il comportamento in regimi di training estremamente lunghi ($>$1000 epoche) rimangono da caratterizzare.
  \item \textbf{Ambito PDE:} Il vantaggio nelle PINN è stato osservato su equazioni con singolarità (Burgers 1D). Su PDE paraboliche totalmente lisce (es. equazione del calore), i risultati preliminari suggeriscono prestazioni comparabili a GELU, non superiori.
\end{itemize}

\subsection{Euristica d'Uso per Ricercatori}

In base alle intuizioni teoriche e ai trend osservati in questa indagine preliminare:

\begin{itemize}
  \item \textbf{Fortemente consigliato:} Architetture LayerNorm / RMSNorm con MLP denso (Vision Transformers, LLM Decoder-only); Scientific Machine Learning su PDE con onde d'urto o singolarità.
  \item \textbf{Vantaggio modesto:} Architetture convolutive con LayerNorm (es. ConvNeXt), dove il bias induttivo spaziale riduce la dipendenza dall'attivazione.
  \item \textbf{Neutro:} Modelli generativi a diffusione con backbone Transformer (DiT), dove NOVA e GELU sono equivalenti.
  \item \textbf{Sconsigliato:} Reti con forte uso di Batch Normalization (es. ResNet, U-Net DDPM con BatchNorm).
\end{itemize}

% ============================================================
\begin{thebibliography}{9}

\bibitem{he2015delving}
He, K., Zhang, X., Ren, S., \& Sun, J. (2015).
Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification.
\textit{Proceedings of the IEEE International Conference on Computer Vision}, pp.~1026--1034.

\bibitem{hendrycks2016gaussian}
Hendrycks, D., \& Gimpel, K. (2016).
Gaussian error linear units (GELUs).
\textit{arXiv preprint arXiv:1606.08415}.

\bibitem{ramachandran2017searching}
Ramachandran, P., Zoph, B., \& Le, Q.\,V. (2017).
Searching for activation functions.
\textit{arXiv preprint arXiv:1710.05941}.

\bibitem{misra2019mish}
Misra, D. (2019).
Mish: A self regularized non-monotonic activation function.
\textit{arXiv preprint arXiv:1908.08681}.

\bibitem{ziyin2020neural}
Ziyin, L., Hartwig, T., \& Ueda, M. (2020).
Neural networks fail to learn periodic functions and how to fix it.
\textit{Advances in Neural Information Processing Systems}, 33, 1583--1594.

\bibitem{raissi2019physics}
Raissi, M., Perdikaris, P., \& Karniadakis, G.\,E. (2019).
Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations.
\textit{Journal of Computational Physics}, 378, 686--707.

\bibitem{touvron2021training}
Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., \& J\'{e}gou, H. (2021).
Training data-efficient image transformers \& distillation through attention.
\textit{Proceedings of the International Conference on Machine Learning (ICML)}, pp.~10347--10357.

\bibitem{cubuk2020randaugment}
Cubuk, E.\,D., Zoph, B., Shlens, J., \& Le, Q.\,V. (2020).
RandAugment: Practical automated data augmentation with a reduced search space.
\textit{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops}, pp.~702--703.

\bibitem{yun2019cutmix}
Yun, S., Han, D., Oh, S.\,J., Chun, S., Choe, J., \& Yoo, Y. (2019).
CutMix: Regularization strategy to train strong classifiers with localizable features.
\textit{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pp.~6023--6032.

\bibitem{zhang2018mixup}
Zhang, H., Cisse, M., Dauphin, Y.\,N., \& Lopez-Paz, D. (2018).
Mixup: Beyond empirical risk minimization.
\textit{International Conference on Learning Representations (ICLR)}.

\bibitem{huang2016deep}
Huang, G., Sun, Y., Liu, Z., Sedra, D., \& Weinberger, K.\,Q. (2016).
Deep networks with stochastic depth.
\textit{European Conference on Computer Vision (ECCV)}, pp.~646--661.

\bibitem{liu2022convnet}
Liu, Z., Mao, H., Wu, C.\,Y., Feichtenhofer, C., Darrell, T., \& Xie, S. (2022).
A ConvNet for the 2020s.
\textit{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pp.~11976--11986.

\bibitem{peebles2023scalable}
Peebles, W., \& Xie, S. (2023).
Scalable diffusion models with transformers.
\textit{Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}, pp.~4195--4205.

\bibitem{nichol2021improved}
Nichol, A.\,Q., \& Dhariwal, P. (2021).
Improved denoising diffusion probabilistic models.
\textit{Proceedings of the International Conference on Machine Learning (ICML)}, pp.~8162--8171.

\bibitem{song2020denoising}
Song, J., Meng, C., \& Ermon, S. (2021).
Denoising diffusion implicit models.
\textit{International Conference on Learning Representations (ICLR)}.

\end{thebibliography}

\end{document}
